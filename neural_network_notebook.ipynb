{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, input_shape, units, drop_prob=0):\n",
    "        \"\"\"\n",
    "        units: (10, 11, 12)\n",
    "            => 3 couches avec 10 neurones pour la premiere\n",
    "        \"\"\"\n",
    "        nb_output = input_shape\n",
    "        self.weights = []\n",
    "        for nb_neurones in units:\n",
    "            self.weights.append(np.random.rand(nb_output, nb_neurones))\n",
    "            self.weights.append(np.zeros(nb_neurones))\n",
    "            nb_output = nb_neurones\n",
    "        \n",
    "        # TODO: a finir\n",
    "        self.drop_prob = drop_prob\n",
    "    \n",
    "    def print_weights(self):\n",
    "        for l in self.weights:\n",
    "            print(l)\n",
    "\n",
    "    def preactiv(self, x):\n",
    "        res = x\n",
    "        for i in range(0, len(self.weights), 2):\n",
    "            res = self.activ(res @ self.weights[i] + self.weights[i + 1])\n",
    "\n",
    "        return res\n",
    "\n",
    "    def activ(self, res):\n",
    "        \"\"\"\n",
    "        :param numpy_array res: output d'une couche\n",
    "        :return l'activation LeakyReLu du resultat\n",
    "        \"\"\"\n",
    "        res[res < 0] *= 0.001\n",
    "        return res\n",
    "\n",
    "    def loss(self, res, y):\n",
    "        if len(y.shape) == 1:\n",
    "            return res - np.expand_dims(y, 1)\n",
    "        return res - y\n",
    "\n",
    "    def d_activ(self, res):\n",
    "        \"\"\"\n",
    "        :param numpy_array res: output d'une couche\n",
    "        :return la derive de l'activation LeakyReLu du resultat\n",
    "        \"\"\"\n",
    "        dres = res\n",
    "        dres[dres < 0] = 0.001\n",
    "        dres[dres >= 0] = 1\n",
    "        return dres\n",
    "    \n",
    "    def forward(self, x, res_arr=None):\n",
    "        \"\"\"\n",
    "        :param numpy_array x: input\n",
    "        :param numpy_array res_arr: array ou on stock les resultats des couches intermediaires\n",
    "        :return le resultat du forward et les resultats intermediaires\n",
    "        \"\"\"\n",
    "        if res_arr:\n",
    "            res_arr[0] = x\n",
    "        res = x\n",
    "        for h in range(0, len(self.weights), 2):  # calculate the result for the current features\n",
    "            res = self.activ(res @ self.weights[h] + self.weights[h + 1])\n",
    "            if res_arr:\n",
    "                res_arr[h//2+1] = res\n",
    "        \n",
    "        if res_arr:\n",
    "            return res, res_arr\n",
    "        else:\n",
    "            return res\n",
    "    \n",
    "    def backward(self, res, y, res_arr, batch_gradient, learning_rate, batch_size):\n",
    "        \"\"\"\n",
    "        :param numpy_array res: resultat du forward\n",
    "        :param numpy_array y: vraie resultat\n",
    "        :param numpy_array res_arr: array ou on stock les resultats des couches intermediaires\n",
    "        :param numpy_array batch_gradient: array contenant la somme des gradients calcules sur le batch\n",
    "        :param integer float: learning rate\n",
    "        :param integer batch_size: taille du batch\n",
    "        \"\"\"\n",
    "        \n",
    "        # calcul de l'erreur finale\n",
    "        # TODO: adapter pour que ca fonctionne avec plusieurs outputs\n",
    "        loss = res - np.expand_dims(y, 1)\n",
    "\n",
    "        # update of the last neurone\n",
    "        delta = loss * self.d_activ(res)\n",
    "        batch_gradient[-2] += res_arr[-2].T @ delta\n",
    "        # TODO: faire fonctionner le bias\n",
    "        batch_gradient[-1] += np.sum(delta)\n",
    "\n",
    "        for b in reversed(range(0, len(self.weights) - 2, 2)):  # backpropagations\n",
    "            l_output = res_arr[(b // 2) + 1]  # layer output\n",
    "            loss = delta @ self.weights[b + 2].T\n",
    "            delta = loss * self.d_activ(l_output)\n",
    "            batch_gradient[b] += res_arr[b//2].T @ delta\n",
    "            # TODO: faire fonctionner le bias\n",
    "            batch_gradient[b+1] += np.sum(delta)\n",
    "\n",
    "        for k in range(len(batch_gradient)):\n",
    "            self.weights[k] -= learning_rate * (1 / batch_size) * batch_gradient[k]\n",
    "    \n",
    "    def fit(self, x, y, step, epochs, batch_size, learning_rate, validation_datas):\n",
    "        \"\"\"\n",
    "        :param numpy_array x: input\n",
    "        :param numpy_array y: vraie resultat\n",
    "        :param numpy_array res_arr: array ou on stock les resultats des couches intermediaires\n",
    "        :param numpy_array batch_gradient: array contenant la somme des gradients calcules sur le batch\n",
    "        :param integer float: learning rate\n",
    "        :param integer batch_size: taille du batch\n",
    "        \"\"\"\n",
    "        x_val = validation_datas[0]\n",
    "        y_val = validation_datas[1]\n",
    "        x = x\n",
    "        y = y\n",
    "        i = 0\n",
    "        epo = 0\n",
    "        steps_per_epoch = step // epochs\n",
    "\n",
    "        default_batch_gradient = []\n",
    "        for k in range(0, len(self.weights), 1):\n",
    "            default_batch_gradient.append(np.zeros(self.weights[k].shape))\n",
    "        \n",
    "        default_res_arr = [np.zeros((self.weights[0].shape[0], batch_size))]\n",
    "        for k in range(0, len(self.weights), 2):\n",
    "            default_res_arr.append(np.zeros((self.weights[k].shape[1], batch_size)))\n",
    "\n",
    "        for etape in tqdm(range(step)):\n",
    "            i += 1\n",
    "            \n",
    "            # recuperation des donnees du batch\n",
    "            bx = x.iloc[batch_size * i : batch_size * (i + 1)].to_numpy()\n",
    "            by = y.iloc[batch_size * i : batch_size * (i + 1)].to_numpy()\n",
    "            \n",
    "            # store the gradient calculated osn the whole batch\n",
    "            batch_gradient = copy.deepcopy(default_batch_gradient)\n",
    "            # store the result after each layer\n",
    "            res_arr = copy.deepcopy(default_res_arr)\n",
    "            \n",
    "            res, res_arr = self.forward(bx, res_arr)\n",
    "            \n",
    "            self.backward(res, by, res_arr, batch_gradient, learning_rate, batch_size)\n",
    "\n",
    "            if x.shape[0] < batch_size * (i + 1) + 1:\n",
    "                p = np.random.permutation(len(x))\n",
    "                x = x.iloc[p]\n",
    "                y = y.iloc[p]\n",
    "                i = 0\n",
    "                \n",
    "            if etape != 0 and etape % (steps_per_epoch) == 0:\n",
    "                predic_val = self.forward(x_val)[0]\n",
    "                predic_train = self.forward(x.iloc[:step // epochs])\n",
    "                print(\"Resume des erreurs sur la validation\")\n",
    "                print(self.loss(predic_val - y_val).describe())\n",
    "        \n",
    "                val_cost = ((self.loss(predic_val, y_val)) ** 2).mean()\n",
    "                train_cost = ((self.loss(predic_train, y.iloc[:step // epochs])) ** 2).mean()\n",
    "                \n",
    "                val_rmse = math.sqrt(val_cost)\n",
    "                train_rmse = math.sqrt(train_cost)\n",
    "                \n",
    "                print(\n",
    "                    \"Epoch: \",\n",
    "                    epo,\n",
    "                    \"| loss val: \",\n",
    "                    val_rmse,\n",
    "                    \" | loss train \",\n",
    "                    train_rmse,\n",
    "                )\n",
    "                print(\"mean predic: \", predic_val.mean(), \" / \", y_val.mean())\n",
    "                \n",
    "                epo += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperation des donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = pd.read_csv(\"./winequality-red.csv\")\n",
    "\n",
    "# train = dataset\n",
    "# validation = dataset.tail(199)\n",
    "\n",
    "# x_train = train.drop('quality', 1)\n",
    "# y_train = train.quality\n",
    "\n",
    "# train_sata = pd.read_csv(\"./train_satander.csv\")\n",
    "# x_train = train_sata.drop(\"target\", 1)\n",
    "# x_train = x_train.drop(\"ID_code\", 1)\n",
    "# y_train = train_sata.target\n",
    "\n",
    "cali_dataframe = pd.read_csv(\"./california_housing_train.csv\")\n",
    "x = cali_dataframe.drop(\"median_house_value\", 1)\n",
    "y = cali_dataframe.median_house_value / 100000\n",
    "\n",
    "# wine_dataframe = pd.read_csv(\"./winequality-red.csv\")\n",
    "# x_train = wine_dataframe.drop(\"quality\", 1)\n",
    "# y_train = wine_dataframe.quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation et transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "\n",
    "for k in x:\n",
    "    x[k] = normalize(x[k])\n",
    "\n",
    "x_train = x.head(16000)\n",
    "y_train = y.head(16000)\n",
    "\n",
    "x_val = x.tail(1000)\n",
    "y_val = y.tail(1000)\n",
    "print(x_val.describe())\n",
    "\n",
    "print(\"y summary\")\n",
    "print(y_train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(x_train.shape[1], (50, 50, 50, 1), drop_prob=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"first predic: \", math.sqrt( ((model.loss(model.forward(x_val), y_val))**2).mean()))\n",
    "print(\"Training\")\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    step=100000,\n",
    "    epochs=10,\n",
    "    batch_size=50,\n",
    "    learning_rate=0.000000001,\n",
    "    validation_datas=(x_val, y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
