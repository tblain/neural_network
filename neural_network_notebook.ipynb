{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-28bf1fd87c1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, Lambda, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import Model\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.python.data import Dataset\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, input_shape, units):\n",
    "        \"\"\"\n",
    "        units: (10, 11, 12)\n",
    "            => 3 couches avec 10 neurones pour la premiere\n",
    "        \"\"\"\n",
    "        nb_output = input_shape\n",
    "        self.weights = []\n",
    "        for nb_neurones in units:\n",
    "            self.weights.append(np.random.rand(nb_output, nb_neurones))\n",
    "            self.weights.append(np.zeros(nb_neurones))\n",
    "            nb_output = nb_neurones\n",
    "    \n",
    "    def print_weights(self):\n",
    "        for l in self.weights:\n",
    "            print(l)\n",
    "\n",
    "    def preactiv(self, x):\n",
    "        res = x\n",
    "        for i in range(0, len(self.weights), 2):\n",
    "            res = self.activ(res @ self.weights[i] + self.weights[i + 1])\n",
    "\n",
    "        return res\n",
    "\n",
    "    def activ(self, res):\n",
    "        res[res < 0] *= 0.2\n",
    "        return res\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.activ(self.preactiv(x))\n",
    "\n",
    "    def predict_on_dataset(self, x):\n",
    "        res = np.zeros(len(x))\n",
    "        for i in range(len(x)):\n",
    "            res[i] = self.predict(x.iloc[i])\n",
    "        return res\n",
    "\n",
    "    def loss(self, y, res):  # 1 example\n",
    "        return res - y\n",
    "\n",
    "    def d_activation(self, res):\n",
    "        res2 = res\n",
    "        res2[res < 0] = 0.2\n",
    "        res2[res >= 0] = 1\n",
    "        return res2\n",
    "    \n",
    "    def cost(self, x, y):  # dataset\n",
    "        total_cost = 0\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            features = x.iloc(i)\n",
    "            target = y.iloc(i)\n",
    "            total_cost += (target - self.predict(features)) ** 2\n",
    "\n",
    "        return total_cost\n",
    "\n",
    "    def fit(self, x, y, step, epochs, batch_size, learning_rate, validation_datas):\n",
    "        x_val = validation_datas[0]\n",
    "        y_val = validation_datas[1]\n",
    "        x = x\n",
    "        y = y\n",
    "        i = 0\n",
    "        epo = 0\n",
    "        steps_per_epoch = step // epochs\n",
    "\n",
    "        default_batch_gradient = []\n",
    "        for k in range(0, len(self.weights), 2):\n",
    "            default_batch_gradient.append(np.zeros(self.weights[k].shape))\n",
    "            default_batch_gradient.append(np.zeros(self.weights[k + 1].shape))\n",
    "\n",
    "        # print(\"default_batch_gradient shape: \")\n",
    "\n",
    "        # for q in range(len(default_batch_gradient)):\n",
    "        #    print(default_batch_gradient[q].shape)\n",
    "\n",
    "        for etape in tqdm(range(step)):\n",
    "            i += 1\n",
    "            bx = x.iloc[batch_size * i : batch_size * (i + 1) - 1]\n",
    "            by = y.iloc[batch_size * i : batch_size * (i + 1) - 1]\n",
    "\n",
    "            batch_gradient = default_batch_gradient # store the gradient calculated on the whole batch\n",
    "\n",
    "            for k in range(len(bx)):  # iterate over the batch\n",
    "                res_l = [bx.iloc[k]]  # store the result after each layer\n",
    "                res = bx.iloc[k]\n",
    "                for h in range(0, len(self.weights), 2):  # calculate the result for the current features\n",
    "                    pre_a = res @ self.weights[h] + self.weights[h + 1]\n",
    "                    res_l.append(pre_a)  # and store the intermediate result before activation\n",
    "                    res = self.activ(pre_a)\n",
    "\n",
    "                loss = self.loss(by.iloc[k], res)\n",
    "                \n",
    "                # update of the last neurone\n",
    "                act_deriv = self.d_activation(res)\n",
    "                batch_gradient[-2] += self.activ(np.expand_dims(res_l[-2], 1)) * loss\n",
    "                batch_gradient[-1] += loss * act_deriv\n",
    "                \n",
    "                for b in reversed(range(0, len(self.weights) - 2, 2)):  # backpropagation\n",
    "                    l_output = res_l[(b // 2) + 1]  # layer output\n",
    "                    loss = self.activ(l_output) * np.dot(loss, self.weights[b + 2].T)\n",
    "                    batch_gradient[b] = l_output * loss * self.d_activation(l_output)\n",
    "#                     print(batch_gradient[b])\n",
    "                    batch_gradient[b + 1] = l_output\n",
    "                    # print(l_output * loss)\n",
    "\n",
    "            for k in range(0, len(self.weights), 2):\n",
    "                self.weights[k] -= learning_rate * (1 / batch_size) * batch_gradient[k]\n",
    "                self.weights[k + 1] -= learning_rate * (1 / batch_size) * batch_gradient[k + 1]\n",
    "\n",
    "            if x.shape[0] < batch_size * (i + 1):\n",
    "                p = np.random.permutation(len(x))\n",
    "                x = x.iloc[p]\n",
    "                y = y.iloc[p]\n",
    "                i = 0\n",
    "\n",
    "            if etape != 0 and etape % (steps_per_epoch) == 0:\n",
    "                print(\"=================\")\n",
    "                predic_val = self.predict_on_dataset(x_val)\n",
    "#                 predic_train = self.predict_on_dataset(bx)\n",
    "                predic_train = self.predict_on_dataset(x.iloc[i : i + (step // epochs)])\n",
    "                # print(\"predic_train: \", predic_train)\n",
    "                print(\n",
    "                    \"Epoch: \",\n",
    "                    epo,\n",
    "                    \"| loss val: \",\n",
    "                    ((predic_val - y_val) ** 2).mean(),\n",
    "                    \" | loss train \",\n",
    "                    ((predic_train - y.iloc[i : i + (step // epochs)]) ** 2).mean() / 2,\n",
    "                )\n",
    "                print(\"mean predic: \", predic_val.mean())\n",
    "                for l in self.weights:\n",
    "#                     print(l)\n",
    "                    pass\n",
    "                epo += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperation des donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = pd.read_csv(\"./winequality-red.csv\")\n",
    "\n",
    "# train = dataset\n",
    "# validation = dataset.tail(199)\n",
    "\n",
    "# x_train = train.drop('quality', 1)\n",
    "# y_train = train.quality\n",
    "\n",
    "# train_sata = pd.read_csv(\"./train_satander.csv\")\n",
    "# x_train = train_sata.drop(\"target\", 1)\n",
    "# x_train = x_train.drop(\"ID_code\", 1)\n",
    "# y_train = train_sata.target\n",
    "\n",
    "cali_dataframe = pd.read_csv(\"./california_housing_train.csv\")\n",
    "x_train = cali_dataframe.drop(\"median_house_value\", 1)\n",
    "y_train = cali_dataframe.median_house_value / 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation et transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         longitude     latitude  housing_median_age  total_rooms  \\\n",
      "count  1000.000000  1000.000000         1000.000000  1000.000000   \n",
      "mean      0.149141     0.652423            0.601255     0.064603   \n",
      "std       0.053452     0.118337            0.265878     0.040652   \n",
      "min       0.000000     0.527099            0.019608     0.000158   \n",
      "25%       0.143426     0.556854            0.352941     0.040464   \n",
      "50%       0.170319     0.618491            0.588235     0.056715   \n",
      "75%       0.185259     0.691817            0.862745     0.076690   \n",
      "max       0.190239     1.000000            1.000000     0.340055   \n",
      "\n",
      "       total_bedrooms   population   households  median_income  \n",
      "count     1000.000000  1000.000000  1000.000000    1000.000000  \n",
      "mean         0.075392     0.032423     0.073566       0.228091  \n",
      "std          0.045796     0.021597     0.046360       0.122239  \n",
      "min          0.000000     0.000280     0.000000       0.000000  \n",
      "25%          0.046671     0.019823     0.046210       0.147688  \n",
      "50%          0.065410     0.027621     0.063723       0.212052  \n",
      "75%          0.091946     0.039743     0.089295       0.278358  \n",
      "max          0.356766     0.172566     0.361618       1.000000  \n",
      "y summary\n",
      "count    17000.000000\n",
      "mean         2.073009\n",
      "std          1.159838\n",
      "min          0.149990\n",
      "25%          1.194000\n",
      "50%          1.804000\n",
      "75%          2.650000\n",
      "max          5.000010\n",
      "Name: median_house_value, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    return (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "\n",
    "for x in x_train:\n",
    "    x_train[x] = normalize(x_train[x])\n",
    "    pass\n",
    "\n",
    "x_val = x_train.tail(1000)\n",
    "y_val = y_train.tail(1000)\n",
    "print(x_val.describe())\n",
    "\n",
    "print(\"y summary\")\n",
    "print(y_train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = layers.Input(shape=(80,))\n",
    "\n",
    "x = layers.BatchNormalization()(input)\n",
    "x = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01))(input)\n",
    "x = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01))(x)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dropout(0.25)(x)\n",
    "x = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01))(x)\n",
    "x = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01))(x)\n",
    "x = layers.Dropout(0.25)(x)\n",
    "# Create output layer with a single node and sigmoid activation\n",
    "output = layers.Dense(1, kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01))(x)\n",
    "model = Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/10000 [00:00<02:33, 64.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first predic:  [23.83220874]\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 491/10000 [00:05<01:34, 100.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 518/10000 [00:05<02:41, 58.73it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | loss val:  7.546738303633252  | loss train  1.754374815202586\n",
      "mean predic:  -0.10501968050867089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 997/10000 [00:10<01:39, 90.39it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1017/10000 [00:11<02:38, 56.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 | loss val:  14.974916231730774  | loss train  6.423063928009531\n",
      "mean predic:  -1.2820375163962658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1493/10000 [00:16<01:36, 88.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 1518/10000 [00:17<02:30, 56.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 | loss val:  15.497290854990917  | loss train  6.921612914356166\n",
      "mean predic:  -1.3508042559212645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2009/10000 [00:22<02:21, 56.56it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "Epoch:  3 | loss val:  8.488316567015422  | loss train  3.4912957179408624\n",
      "mean predic:  -0.28325678226065176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 2490/10000 [00:27<01:35, 78.72it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2508/10000 [00:28<02:20, 53.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4 | loss val:  424.0677650565682  | loss train  207.44893517664818\n",
      "mean predic:  22.619980022597577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 2998/10000 [00:33<01:23, 84.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3014/10000 [00:34<02:15, 51.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5 | loss val:  4.7605366989240885  | loss train  2.1556028102038645\n",
      "mean predic:  0.5651725355440089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 3493/10000 [00:39<01:12, 89.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "Epoch:  6 | loss val:  14.416077448176592  | loss train  5.96046917028951\n",
      "mean predic:  -1.2069617960822685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 3991/10000 [00:45<01:01, 97.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4018/10000 [00:46<01:53, 52.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7 | loss val:  15.843873003914268  | loss train  6.5649154209212535\n",
      "mean predic:  -1.3957195856519797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 4512/10000 [00:52<01:39, 55.14it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "Epoch:  8 | loss val:  9.277224411655103  | loss train  3.531602582593885\n",
      "mean predic:  -0.4238455825848736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5006/10000 [00:57<01:27, 57.30it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "Epoch:  9 | loss val:  381.3886265641469  | loss train  180.45976279261157\n",
      "mean predic:  21.56845751048471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 5508/10000 [01:03<01:43, 43.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "Epoch:  10 | loss val:  7.707188900380408  | loss train  3.836095686660554\n",
      "mean predic:  4.841201224889128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 5993/10000 [01:08<00:40, 97.92it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6011/10000 [01:09<01:13, 54.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11 | loss val:  13.635693062246643  | loss train  5.543556844508945\n",
      "mean predic:  -1.0994539401775305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6502/10000 [01:15<01:23, 41.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "Epoch:  12 | loss val:  16.058160811208072  | loss train  7.020416463989943\n",
      "mean predic:  -1.4232537518885824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7000/10000 [01:20<00:32, 92.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7010/10000 [01:21<01:03, 46.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13 | loss val:  10.168514807367217  | loss train  4.210324269249044\n",
      "mean predic:  -0.5746640381281696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 7491/10000 [01:26<00:26, 93.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 7510/10000 [01:27<00:43, 57.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14 | loss val:  299.7051739600131  | loss train  145.62954582813836\n",
      "mean predic:  19.37804923786007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 7993/10000 [01:32<00:20, 99.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8012/10000 [01:32<00:36, 54.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15 | loss val:  49.17648420254879  | loss train  23.75623445618851\n",
      "mean predic:  9.174099453453694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 8495/10000 [01:38<00:14, 106.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 8515/10000 [01:38<00:22, 65.49it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  16 | loss val:  12.745350756764969  | loss train  5.314046015592676\n",
      "mean predic:  -0.9726128844851608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 8990/10000 [01:43<00:10, 95.46it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9024/10000 [01:44<00:12, 79.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17 | loss val:  16.17861008901815  | loss train  6.997349270155349\n",
      "mean predic:  -1.438687996404754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 9496/10000 [01:48<00:04, 109.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 9519/10000 [01:48<00:06, 71.33it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  18 | loss val:  11.11972590882077  | loss train  4.596493753462482\n",
      "mean predic:  -0.727572784935609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:52<00:00, 88.51it/s]\n"
     ]
    }
   ],
   "source": [
    "model = NN(x_train.shape[1], (10, 10, 1))\n",
    "print(\"first predic: \", model.predict(x_train.iloc[0]))\n",
    "\n",
    "# print(\"starting weights\")\n",
    "# model.print_weights()\n",
    "\n",
    "print(\"Training\")\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    step=10000,\n",
    "    epochs=20,\n",
    "    batch_size=20,\n",
    "    learning_rate=0.00000005,\n",
    "    validation_datas=(x_val, y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
