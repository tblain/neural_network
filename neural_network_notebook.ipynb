{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, input_shape, units):\n",
    "        \"\"\"\n",
    "        units: (10, 11, 12)\n",
    "            => 3 couches avec 10 neurones pour la premiere\n",
    "        \"\"\"\n",
    "        nb_output = input_shape\n",
    "        self.weights = []\n",
    "        for nb_neurones in units:\n",
    "            self.weights.append(np.random.rand(nb_output, nb_neurones))\n",
    "            self.weights.append(np.zeros(nb_neurones))\n",
    "            nb_output = nb_neurones\n",
    "    \n",
    "    def print_weights(self):\n",
    "        for l in self.weights:\n",
    "            print(l)\n",
    "\n",
    "    def preactiv(self, x):\n",
    "        res = x\n",
    "        for i in range(0, len(self.weights), 2):\n",
    "            res = self.activ(res @ self.weights[i] + self.weights[i + 1])\n",
    "\n",
    "        return res\n",
    "\n",
    "    def activ(self, res):\n",
    "        res[res < 0] *= 0.001\n",
    "        return res\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.activ(self.preactiv(x))\n",
    "\n",
    "    def predict_on_dataset(self, x):\n",
    "        res = np.zeros(len(x))\n",
    "        for i in range(len(x)):\n",
    "            res[i] = self.predict(x.iloc[i])\n",
    "        return res\n",
    "\n",
    "    def loss(self, y, res):  # 1 example\n",
    "        return res - y\n",
    "\n",
    "    def d_activ(self, res):\n",
    "        dres = res\n",
    "        dres[dres < 0] = 0.001\n",
    "        dres[dres >= 0] = 1\n",
    "        return dres\n",
    "    \n",
    "    def fit(self, x, y, step, epochs, batch_size, learning_rate, validation_datas):\n",
    "        x_val = validation_datas[0]\n",
    "        y_val = validation_datas[1]\n",
    "        x = x\n",
    "        y = y\n",
    "        i = 0\n",
    "        epo = 0\n",
    "        steps_per_epoch = step // epochs\n",
    "\n",
    "        default_batch_gradient = []\n",
    "        for k in range(0, len(self.weights), 1):\n",
    "            default_batch_gradient.append(np.zeros(self.weights[k].shape))\n",
    "        \n",
    "        default_res_arr = [np.zeros((self.weights[0].shape[0], batch_size))]\n",
    "        for k in range(0, len(self.weights), 2):\n",
    "            default_res_arr.append(np.zeros((self.weights[k].shape[1], batch_size)))\n",
    "\n",
    "        for etape in tqdm(range(step)):\n",
    "            i += 1\n",
    "            \n",
    "            bx = x.iloc[batch_size * i : batch_size * (i + 1)].to_numpy()\n",
    "            by = y.iloc[batch_size * i : batch_size * (i + 1)].to_numpy()\n",
    "            \n",
    "            # store the gradient calculated on the whole batch\n",
    "            batch_gradient = copy.deepcopy(default_batch_gradient)\n",
    "            # store the result after each layer\n",
    "            res_arr = copy.deepcopy(default_res_arr)\n",
    "            \n",
    "            res_arr[0] = bx  \n",
    "            res = bx\n",
    "            for h in range(0, len(self.weights), 2):  # calculate the result for the current features\n",
    "                res = self.activ(res @ self.weights[h] + self.weights[h + 1])\n",
    "                res_arr[h//2+1] = res\n",
    "\n",
    "            loss = res - np.expand_dims(by, 1)\n",
    "\n",
    "            # update of the last neurone\n",
    "            delta = loss * self.d_activ(res)\n",
    "            batch_gradient[-2] += res_arr[-2].T @ delta\n",
    "            # TODO: faire fonctionner le bias\n",
    "\n",
    "            for b in reversed(range(0, len(self.weights) - 2, 2)):  # backpropagation\n",
    "                l_output = res_arr[(b // 2) + 1]  # layer output\n",
    "                loss = delta @ self.weights[b + 2].T\n",
    "                delta = loss * self.d_activ(l_output)\n",
    "                batch_gradient[b] += res_arr[b//2].T @ delta\n",
    "                # TODO: faire fonctionner le bias\n",
    "#                 batch_gradient[b+1] += delta.mean()\n",
    "                pass\n",
    "\n",
    "#             return\n",
    "\n",
    "            for k in range(len(batch_gradient)):\n",
    "                self.weights[k] -= learning_rate * (1 / batch_size) * batch_gradient[k]\n",
    "\n",
    "            if x.shape[0] < batch_size * (i + 1) + 1:\n",
    "                p = np.random.permutation(len(x))\n",
    "                x = x.iloc[p]\n",
    "                y = y.iloc[p]\n",
    "                i = 0\n",
    "\n",
    "            if etape != 0 and etape % (steps_per_epoch) == 0:\n",
    "                predic_val = self.predict_on_dataset(x_val)\n",
    "                predic_train = self.predict_on_dataset(x.iloc[i : i + (step // epochs)])\n",
    "        \n",
    "                val_cost = ((predic_val - y_val) ** 2).mean()\n",
    "                train_cost = ((predic_train - y.iloc[i : i + (step // epochs)]) ** 2).mean()\n",
    "                \n",
    "                val_rmse = math.sqrt(val_cost)\n",
    "                train_rmse = math.sqrt(train_cost)\n",
    "                \n",
    "                print(\n",
    "                    \"Epoch: \",\n",
    "                    epo,\n",
    "                    \"| loss val: \",\n",
    "                    val_rmse,\n",
    "                    \" | loss train \",\n",
    "                    train_rmse,\n",
    "                )\n",
    "                print(\"mean predic: \", predic_val.mean(), \" / \", y_val.mean())\n",
    "                \n",
    "                epo += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperation des donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = pd.read_csv(\"./winequality-red.csv\")\n",
    "\n",
    "# train = dataset\n",
    "# validation = dataset.tail(199)\n",
    "\n",
    "# x_train = train.drop('quality', 1)\n",
    "# y_train = train.quality\n",
    "\n",
    "# train_sata = pd.read_csv(\"./train_satander.csv\")\n",
    "# x_train = train_sata.drop(\"target\", 1)\n",
    "# x_train = x_train.drop(\"ID_code\", 1)\n",
    "# y_train = train_sata.target\n",
    "\n",
    "cali_dataframe = pd.read_csv(\"./california_housing_train.csv\")\n",
    "x_train = cali_dataframe.drop(\"median_house_value\", 1)\n",
    "y_train = cali_dataframe.median_house_value / 1000\n",
    "\n",
    "# wine_dataframe = pd.read_csv(\"./winequality-red.csv\")\n",
    "# x_train = wine_dataframe.drop(\"quality\", 1)\n",
    "# y_train = wine_dataframe.quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation et transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "\n",
    "for x in x_train:\n",
    "    x_train[x] = normalize(x_train[x])\n",
    "    pass\n",
    "\n",
    "x_val = x_train.tail(500)\n",
    "y_val = y_train.tail(500)\n",
    "print(x_val.describe())\n",
    "\n",
    "print(\"y summary\")\n",
    "print(y_train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(x_train.shape[1], (20, 20, 20, 1))\n",
    "print(\"first predic: \", math.sqrt( ((model.predict_on_dataset(x_val) - y_val)**2).mean()))\n",
    "\n",
    "# print(\"starting weights\")\n",
    "# model.print_weights()\n",
    "\n",
    "print(\"Training\")\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    step=100000,\n",
    "    epochs=10,\n",
    "    batch_size=50,\n",
    "    learning_rate=0.000001,\n",
    "    validation_datas=(x_val, y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
