{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, input_shape, units, drop_prob=0):\n",
    "        \"\"\"\n",
    "        units: (10, 11, 12)\n",
    "            => 3 couches avec 10 neurones pour la premiere\n",
    "        \"\"\"\n",
    "        nb_output = input_shape\n",
    "        self.weights = []\n",
    "        for nb_neurones in units:\n",
    "            self.weights.append(np.random.rand(nb_output, nb_neurones) * 0.1)\n",
    "            self.weights.append(np.zeros(nb_neurones))\n",
    "            nb_output = nb_neurones\n",
    "        \n",
    "        # TODO: a finir\n",
    "        self.drop_prob = drop_prob\n",
    "        self.drop_out_layers = []\n",
    "\n",
    "#==========================================================================================================\n",
    "    \n",
    "    def print_weights(self):\n",
    "        for l in self.weights:\n",
    "            print(l)\n",
    "\n",
    "#==========================================================================================================\n",
    "\n",
    "    def preactiv(self, x):\n",
    "        res = x\n",
    "        for i in range(0, len(self.weights), 2):\n",
    "            res = self.activ(res @ self.weights[i] + self.weights[i + 1])\n",
    "\n",
    "        return res\n",
    "\n",
    "#==========================================================================================================\n",
    "\n",
    "    def activ(self, res):\n",
    "        \"\"\"\n",
    "        :param numpy_array res: output d'une couche\n",
    "        :return l'activation LeakyReLu du resultat\n",
    "        \"\"\"\n",
    "        res[res < 0] *= 0.0001\n",
    "        return res\n",
    "\n",
    "#==========================================================================================================\n",
    "\n",
    "    def loss(self, res, y):\n",
    "        nres = np.array(res, dtype=np.float64)\n",
    "        ny = np.array(y, dtype=np.float64)\n",
    "        if len(y.shape) == 1:\n",
    "            ny = np.expand_dims(y, 1)\n",
    "        if len(res.shape) == 1:\n",
    "            nres = np.expand_dims(res, 1)\n",
    "        return nres - ny\n",
    "\n",
    "#==========================================================================================================\n",
    "\n",
    "    def d_activ(self, res):\n",
    "        \"\"\"\n",
    "        :param numpy_array res: output d'une couche\n",
    "        :return la derive de l'activation LeakyReLu du resultat\n",
    "        \"\"\"\n",
    "        dres = res\n",
    "        dres[dres < 0] = 0.0001\n",
    "        dres[dres >= 0] = 1\n",
    "        return dres\n",
    "\n",
    "#==========================================================================================================\n",
    "    \n",
    "    def forward(self, x, res_arr=None, dropping=False):\n",
    "        \"\"\"\n",
    "        :param numpy_array x: input\n",
    "        :param numpy_array res_arr: array ou on stock les resultats des couches intermediaires\n",
    "        :return le resultat du forward et les resultats intermediaires\n",
    "        \"\"\"\n",
    "        nx = np.array(x, dtype=np.float64)\n",
    "        if res_arr:\n",
    "            res_arr[0] = nx\n",
    "        res = nx\n",
    "        for h in range(0, len(self.weights), 2):  # calculate the result for the current features\n",
    "#             print(\"res: \", res.dtype)\n",
    "            res = self.activ(res @ self.weights[h] + self.weights[h + 1])\n",
    "            if dropping:\n",
    "                res = np.multiply(res, self.drop_out_layers[h//2])\n",
    "                pass\n",
    "            else:\n",
    "                res = np.multiply(res, self.drop_prob)\n",
    "                pass\n",
    "            if res_arr:\n",
    "                res_arr[h//2+1] = res\n",
    "        \n",
    "        if res_arr:\n",
    "            return res, res_arr\n",
    "        else:\n",
    "            return res\n",
    "\n",
    "#==========================================================================================================\n",
    "    \n",
    "    def backward(self, res, y, res_arr, batch_gradient, learning_rate, batch_size):\n",
    "        \"\"\"\n",
    "        :param numpy_array res: resultat du forward\n",
    "        :param numpy_array y: vraie resultat\n",
    "        :param numpy_array res_arr: array ou on stock les resultats des couches intermediaires\n",
    "        :param numpy_array batch_gradient: array contenant la somme des gradients calcules sur le batch\n",
    "        :param integer float: learning rate\n",
    "        :param integer batch_size: taille du batch\n",
    "        \"\"\"\n",
    "        \n",
    "        # calcul de l'erreur finale\n",
    "        # TODO: adapter pour que ca fonctionne avec plusieurs outputs\n",
    "        if len(y.shape) == 1:\n",
    "            loss = res - np.expand_dims(y, 1)\n",
    "        else:\n",
    "            loss = res - y\n",
    "        \n",
    "        \n",
    "        # update of the last neurone\n",
    "        delta = loss * self.d_activ(res)\n",
    "        batch_gradient[-2] = res_arr[-2].T @ delta\n",
    "        batch_gradient[-1] += np.sum(delta)\n",
    "        \n",
    "        #         print(\"bg: \", batch_gradient[-2].shape)\n",
    "        #         print(\"ra-2: \", res_arr[-2].T.shape)\n",
    "        #         print(\"weights: \", self.weights[0].shape)\n",
    "        #         print(\"delta: \", delta.shape)\n",
    "        #         print(\"y: \", y.shape)\n",
    "        #         print(\"res: \", res.shape)\n",
    "        #         print(\"loss: \", loss.shape)\n",
    "        #         print(\"bg: \", batch_gradient[-2])\n",
    "\n",
    "\n",
    "        for b in reversed(range(0, len(self.weights) - 2, 2)):  # backpropagations\n",
    "            l_output = res_arr[(b // 2) + 1]  # layer output\n",
    "            loss = delta @ self.weights[b + 2].T\n",
    "            delta = loss * self.d_activ(l_output)\n",
    "            batch_gradient[b] = res_arr[b//2].T @ delta\n",
    "            batch_gradient[b+1] += np.sum(delta)\n",
    "\n",
    "        for k in range(len(batch_gradient)):\n",
    "            self.weights[k] -= learning_rate * (1 / batch_size) * batch_gradient[k]\n",
    "\n",
    "#==========================================================================================================\n",
    "\n",
    "    def fit(self, x, y, step, epochs, batch_size, learning_rate, validation_datas):\n",
    "        \"\"\"\n",
    "        :param numpy_array x: input\n",
    "        :param numpy_array y: vraie resultat\n",
    "        :param numpy_array res_arr: array ou on stock les resultats des couches intermediaires\n",
    "        :param numpy_array batch_gradient: array contenant la somme des gradients calcules sur le batch\n",
    "        :param integer float: learning rate\n",
    "        :param integer batch_size: taille du batch\n",
    "        \"\"\"\n",
    "        x_val = validation_datas[0]\n",
    "        y_val = validation_datas[1]\n",
    "        \n",
    "        x = x\n",
    "        y = y\n",
    "        \n",
    "        i = 0\n",
    "        epo = 0\n",
    "        steps_per_epoch = step // epochs\n",
    "\n",
    "        default_batch_gradient = []\n",
    "        for k in range(0, len(self.weights), 1):\n",
    "            default_batch_gradient.append(np.zeros(self.weights[k].shape))\n",
    "        \n",
    "        default_res_arr = [np.zeros((self.weights[0].shape[0], batch_size))]\n",
    "        for k in range(0, len(self.weights), 2):\n",
    "            default_res_arr.append(np.zeros((self.weights[k].shape[1], batch_size)))\n",
    "        \n",
    "#         print(self.drop_out_layers)\n",
    "\n",
    "        for etape in tqdm(range(step)):\n",
    "            i += 1\n",
    "            \n",
    "            # recuperation des donnees du batch\n",
    "            bx = x.iloc[batch_size * i : batch_size * (i + 1)].to_numpy()\n",
    "            by = y.iloc[batch_size * i : batch_size * (i + 1)].to_numpy()\n",
    "            \n",
    "            # store the gradient calculated osn the whole batch\n",
    "            batch_gradient = copy.deepcopy(default_batch_gradient)\n",
    "            # store the result after each layer\n",
    "            res_arr = copy.deepcopy(default_res_arr)\n",
    "            \n",
    "            self.drop_out_layers = []\n",
    "            for k in range(0, len(self.weights), 2):\n",
    "                self.drop_out_layers.append(np.random.random(self.weights[k+1].shape) < self.drop_prob)\n",
    "            \n",
    "            res, res_arr = self.forward(bx, res_arr, dropping=True)\n",
    "            \n",
    "            self.backward(res, by, res_arr, batch_gradient, learning_rate, batch_size)\n",
    "\n",
    "            if x.shape[0] < batch_size * (i + 1) + 1:\n",
    "                p = np.random.permutation(len(x))\n",
    "                x = x.iloc[p]\n",
    "                y = y.iloc[p]\n",
    "                i = 0\n",
    "                \n",
    "            if etape != 0 and etape % (steps_per_epoch) == 0:\n",
    "                predic_val = self.forward(x_val)[0]\n",
    "                predic_train = self.forward(x.iloc[:step // epochs])\n",
    "                print(\"Resume des erreurs sur la validation\")\n",
    "                print(pd.DataFrame(self.loss(predic_val, y_val)).describe())\n",
    "                print(\"BG\")\n",
    "                print(batch_gradient[-2][:5])\n",
    "                val_cost = ((self.loss(predic_val, y_val)) ** 2).mean()\n",
    "                train_cost = ((self.loss(predic_train, y.iloc[:step // epochs])) ** 2).mean()\n",
    "                \n",
    "                val_rmse = math.sqrt(val_cost)\n",
    "                train_rmse = math.sqrt(train_cost)\n",
    "                \n",
    "                print(\n",
    "                    \"Epoch: \",\n",
    "                    epo,\n",
    "                    \"| loss val: \",\n",
    "                    val_rmse,\n",
    "                    \" | loss train \",\n",
    "                    train_rmse,\n",
    "                )\n",
    "                print(\"mean predic: \", predic_val.mean(), \" / \", y_val.mean())\n",
    "                \n",
    "                epo += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperation des donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe = pd.read_csv(\"train.csv\")\n",
    "train_dataframe.describe()\n",
    "train_dataframe = train_dataframe.reindex(\n",
    "    np.random.permutation(train_dataframe.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(dataframe):\n",
    "    \"\"\"Prepares input features from Ames housing data set.\n",
    "\n",
    "    Args:\n",
    "    dataframe: A Pandas DataFrame expected to contain data\n",
    "      from the Ames housing data set.\n",
    "    Returns:\n",
    "    A DataFrame that contains the features to be used for the model, including\n",
    "    synthetic features.\n",
    "    \"\"\"\n",
    "    processed_features = dataframe.copy()\n",
    "    \n",
    "    processed_features['MSZoning'] = encode_and_bind(processed_features, 'MSZoning')\n",
    "    processed_features['LotFrontage'] = encode_and_bind(processed_features, 'LotFrontage')\n",
    "    processed_features['Street'] = encode_and_bind(processed_features, 'Street')\n",
    "    processed_features['Alley'] = encode_and_bind(processed_features, 'Alley')\n",
    "    processed_features['LotShape'] = encode_and_bind(processed_features, 'LotShape')\n",
    "    processed_features['LandContour'] = encode_and_bind(processed_features, 'LandContour')\n",
    "    processed_features['Utilities'] = encode_and_bind(processed_features, 'Utilities')\n",
    "    processed_features['LotConfig'] = encode_and_bind(processed_features, 'LotConfig')\n",
    "    processed_features['LandSlope'] = encode_and_bind(processed_features, 'LandSlope')\n",
    "    processed_features['Neighborhood'] = encode_and_bind(processed_features, 'Neighborhood')\n",
    "    processed_features['Condition1'] = encode_and_bind(processed_features, 'Condition1')\n",
    "    processed_features['Condition2'] = encode_and_bind(processed_features, 'Condition2')\n",
    "    processed_features['BldgType'] = encode_and_bind(processed_features, 'BldgType')\n",
    "    processed_features['HouseStyle'] = encode_and_bind(processed_features, 'HouseStyle')\n",
    "    processed_features['RoofStyle'] = encode_and_bind(processed_features, 'RoofStyle')\n",
    "    processed_features['RoofMatl'] = encode_and_bind(processed_features, 'RoofMatl')\n",
    "    processed_features['Exterior1st'] = encode_and_bind(processed_features, 'Exterior1st')\n",
    "    processed_features['Exterior2nd'] = encode_and_bind(processed_features, 'Exterior2nd')\n",
    "    processed_features['MasVnrType'] = encode_and_bind(processed_features, 'MasVnrType')\n",
    "    processed_features['MasVnrArea'] = encode_and_bind(processed_features, 'MasVnrType')\n",
    "    processed_features['ExterQual'] = encode_and_bind(processed_features, 'ExterQual')\n",
    "    processed_features['ExterCond'] = encode_and_bind(processed_features, 'ExterCond')\n",
    "    processed_features['Foundation'] = encode_and_bind(processed_features, 'Foundation')\n",
    "    processed_features['BsmtQual'] = encode_and_bind(processed_features, 'BsmtQual')\n",
    "    processed_features['BsmtCond'] = encode_and_bind(processed_features, 'BsmtCond')\n",
    "    processed_features['BsmtExposure'] = encode_and_bind(processed_features, 'BsmtExposure')\n",
    "    processed_features['BsmtFinType1'] = encode_and_bind(processed_features, 'BsmtFinType1')\n",
    "    processed_features['BsmtFinType2'] = encode_and_bind(processed_features, 'BsmtFinType2')\n",
    "    processed_features['Heating'] = encode_and_bind(processed_features, 'Heating')\n",
    "    processed_features['HeatingQC'] = encode_and_bind(processed_features, 'HeatingQC')\n",
    "    processed_features['CentralAir'] = encode_and_bind(processed_features, 'CentralAir')\n",
    "    processed_features['Electrical'] = encode_and_bind(processed_features, 'Electrical')\n",
    "    processed_features['KitchenQual'] = encode_and_bind(processed_features, 'KitchenQual')\n",
    "    processed_features['Functional'] = encode_and_bind(processed_features, 'Functional')\n",
    "    processed_features['FireplaceQu'] = encode_and_bind(processed_features, 'FireplaceQu')\n",
    "    processed_features['GarageType'] = encode_and_bind(processed_features, 'GarageType')\n",
    "    processed_features['GarageYrBlt'] = encode_and_bind(processed_features, 'GarageYrBlt')\n",
    "    processed_features['GarageFinish'] = encode_and_bind(processed_features, 'GarageFinish')\n",
    "    processed_features['GarageQual'] = encode_and_bind(processed_features, 'GarageQual')\n",
    "    processed_features['GarageCond'] = encode_and_bind(processed_features, 'GarageCond')\n",
    "    processed_features['PavedDrive'] = encode_and_bind(processed_features, 'PavedDrive')\n",
    "    processed_features['PoolQC'] = encode_and_bind(processed_features, 'PoolQC')\n",
    "    processed_features['Fence'] = encode_and_bind(processed_features, 'Fence')\n",
    "    processed_features['MiscFeature'] = encode_and_bind(processed_features, 'MiscFeature')\n",
    "    processed_features['SaleType'] = encode_and_bind(processed_features, 'SaleType')\n",
    "    processed_features['SaleCondition'] = encode_and_bind(processed_features, 'SaleCondition')\n",
    "    selected_features = processed_features#[['MSSubClass', 'MSZoning', 'SaleType', 'SaleCondition', 'LotArea', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood',\n",
    "                                          # 'Condition1', 'Condition2','BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond',\n",
    "                                          #  'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir',\n",
    "                                          #  'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive',\n",
    "                                          #  'PoolQC', 'Fence', 'MiscFeature']]\n",
    "    if 'SalePrice' in list(selected_features):\n",
    "        selected_features = selected_features.drop('SalePrice', 1)\n",
    "    # Create a synthetic feature.\n",
    "#     processed_features[\"rooms_per_person\"] = (\n",
    "#      dataframe[\"total_rooms\"] /\n",
    "#      dataframe[\"population\"])\n",
    "    return selected_features\n",
    "\n",
    "def preprocess_targets(dataframe):\n",
    "    \"\"\"Prepares target features (i.e., labels) from Ames housing data set.\n",
    "\n",
    "    Args:\n",
    "    dataframe: A Pandas DataFrame expected to contain data\n",
    "    from the Ames housing data set.\n",
    "    Returns:\n",
    "    A DataFrame that contains the target feature.\n",
    "    \"\"\"\n",
    "    output_targets = pd.DataFrame()\n",
    "    # Scale the target to be in units of thousands of dollars.\n",
    "    output_targets[\"SalePrice\"] = (\n",
    "      dataframe[\"SalePrice\"] / 1000.0)\n",
    "    return output_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_one_hot(df, cols):\n",
    "    \"\"\"\n",
    "    @param df pandas DataFrame\n",
    "    @param cols a list of columns to encode \n",
    "    @return a DataFrame with one-hot encoding\n",
    "    \"\"\"\n",
    "    for each in cols:\n",
    "        dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df\n",
    "\n",
    "def encode_and_bind(original_dataframe, feature_to_encode):\n",
    "    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n",
    "    res = pd.concat([original_dataframe, dummies], axis=1)\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = pd.read_csv(\"./winequality-red.csv\")\n",
    "\n",
    "# train = dataset\n",
    "# validation = dataset.tail(199)\n",
    "\n",
    "# x_train = train.drop('quality', 1)\n",
    "# y_train = train.quality\n",
    "\n",
    "# train_sata = pd.read_csv(\"./train_satander.csv\")\n",
    "# x_train = train_sata.drop(\"target\", 1)\n",
    "# x_train = x_train.drop(\"ID_code\", 1)\n",
    "# y_train = train_sata.target\n",
    "\n",
    "cali_dataframe = pd.read_csv(\"./california_housing_train.csv\")\n",
    "# x = cali_dataframe.drop(\"median_house_value\", 1)\n",
    "# y = cali_dataframe.median_house_value / 100000\n",
    "\n",
    "# wine_dataframe = pd.read_csv(\"./winequality-red.csv\")\n",
    "# x_train = wine_dataframe.drop(\"quality\", 1)\n",
    "# y_train = wine_dataframe.quality\n",
    "\n",
    "mnist_dataframe = pd.read_csv(\"../kaggle/MNIST/train.csv\")\n",
    "x = mnist_dataframe.drop(\"label\", 1) / 255\n",
    "y = mnist_dataframe.label\n",
    "# x.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation et transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "\n",
    "for k in x:\n",
    "    x[k] = normalize(x[k])\n",
    "    pass\n",
    "\n",
    "x_train = x.head(15000)\n",
    "y_train = y.head(15000)\n",
    "\n",
    "x_val = x.tail(2000)\n",
    "y_val = y.tail(2000)\n",
    "\n",
    "# Choose the first 12000 (out of 17000) examples for training.\n",
    "x_train = preprocess_features(train_dataframe.head(1200))\n",
    "y_train = preprocess_targets(train_dataframe.head(1200))\n",
    "\n",
    "# Choose the last 5000 (out of 17000) examples for validation.\n",
    "x_val = preprocess_features(train_dataframe.tail(260))\n",
    "y_val = preprocess_targets(train_dataframe.tail(260))\n",
    "\n",
    "print(x_val.describe())\n",
    "\n",
    "print(\"y summary\")\n",
    "print(y_train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(x_train.shape[1], (100, 100, 100, 1), drop_prob=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"first predic: \", math.sqrt( ((model.loss(model.forward(x_val), y_val))**2).mean()) )\n",
    "print(\"Shape: \", x_train.shape)\n",
    "print(\"Training\")\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    step=300000,\n",
    "    epochs=50,\n",
    "    batch_size=50,\n",
    "    learning_rate=0.00000000001,\n",
    "    validation_datas=(x_val, y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save = copy.deepcopy(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
