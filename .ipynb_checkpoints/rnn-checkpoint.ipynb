{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 29 characters, 15 unique.\n",
      "----\n",
      " 88jsahnna1ouea ta 8 uo un 88118ahsiae8'tms's 'etmu ia8oi1'hm8ai oajmoeh'n1nsn'timhhaaeimae8taoht8at8'e8o injtu'iejhe1o1m1a'et81n' iuuse'i 'hu1etomtnmi'h1i8ta n1nioii1'e1h nntso8ms81tnhhsttunt1jn1aeshu \n",
      "----\n",
      "iter 0, loss: 67.701261\n",
      "----\n",
      " e suis thomas et j''i 18 jjai 18 t'aie18 j'ai 11 j'ai 18 jiai 18 j'ai 18 jmai 18hj'ai 18 j'ai 18 j'am 18 j'ai 18 j'ai 18 j'ai 1t jmai t8 jjai 18 'mas 18 jiai 18 j'ai 18 j'ai 18 j'as 18 j'ai 1t j'ai 18 \n",
      "----\n",
      "iter 100, loss: 64.352170\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 jmis 18tjmai 18 j'ai 18 j'aie18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 1t j'ais18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 200, loss: 58.277639\n",
      "----\n",
      " e suis thomas et j''i 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 1  j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 300, loss: 52.751872\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 400, loss: 47.743936\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'as 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 500, loss: 43.208893\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 1t j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 600, loss: 39.103329\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 700, loss: 35.387132\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 800, loss: 32.023680\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 900, loss: 28.979683\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 1000, loss: 26.224921\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'oi 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 1100, loss: 23.731982\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 1200, loss: 21.476025\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ji 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 1300, loss: 19.434551\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 1400, loss: 17.587188\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 1500, loss: 15.915492\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 1600, loss: 14.402767\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 1700, loss: 13.033901\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j''i 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 1800, loss: 11.795219\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 1900, loss: 10.674340\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 2000, loss: 9.660064\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 2100, loss: 8.742252\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 2200, loss: 7.911730\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 2300, loss: 7.160196\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 1t j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 2400, loss: 6.480137\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18hj'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 2500, loss: 5.864754\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 2600, loss: 5.307896\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 2700, loss: 4.803995\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 2800, loss: 4.348014\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'mi 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 2900, loss: 3.935395\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 3000, loss: 3.562013\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 jaai 18 j'ai 18 \n",
      "----\n",
      "iter 3100, loss: 3.224135\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 3200, loss: 2.918384\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 3300, loss: 2.641704\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 3400, loss: 2.391329\n",
      "----\n",
      " e suis thomas et j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 j'ai 18 \n",
      "----\n",
      "iter 3500, loss: 2.164758\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2c6e35961be0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iter %d, loss: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-2c6e35961be0>\u001b[0m in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mdWxh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mdWhh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mdhnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "  \n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    \n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"\n",
    "    sample a sequence of integers from the model\n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    \n",
    "    return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "while True:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % 100 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                   [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
